import os
import sys
import traceback
import importlib
import sqlalchemy as sql
from functools import partial
from collections import OrderedDict

from prefect import flow, task, get_run_logger
from prefect.serializers import JSONSerializer
from prefect.task_runners import SequentialTaskRunner
from prefect.filesystems import RemoteFileSystem as RFS
from prefect.context import TaskRunContext, FlowRunContext

from dataflow_ui_plugin.hooks import *
from dataflow_ui_plugin.flowutils import *
from dataflow_ui_plugin.nodes import generate_nodes_flow


def setup_plugin():
    # Setup plugin by adding path to python flow source so that modules from app/pysrc in dataflow-gen-agent container can be imported dynamically
    sys.path.append('/app/pysrc')


@flow(log_prints=True, task_runner=SequentialTaskRunner)
def dataflow_ui_plugin(json_graph, options):
    logger = get_run_logger()
    setup_plugin()

    # Grab root flow id
    root_flow_run_context = FlowRunContext.get().flow_run.dict()
    root_flow_run_id = str(root_flow_run_context.get("id"))

    _options = options
    graph = json_graph
    sorted_nodes = get_node_list(graph)  # array of nodes that is sorted
    get_run_logger().debug(f"Total number of nodes: {len(sorted_nodes)}")
    nodes_out = {}
    testmode = _options["test_mode"]
    trace_config = _options["trace_config"]
    tracemode = trace_config["trace_mode"]

    # Generate nodes in a subflow
    generate_nodes_flow_wo = generate_nodes_flow.with_options(
        on_completion=[
            partial(generate_nodes_flow_hook, **dict(graph=graph, sorted_nodes=sorted_nodes))],
        on_failure=[
            partial(generate_nodes_flow_hook, **dict(graph=graph, sorted_nodes=sorted_nodes))]
    )

    generated_nodes = generate_nodes_flow_wo(graph, sorted_nodes)  # flow

    get_run_logger().debug(f"Graph with nodes: {generated_nodes}")

    # Execute nodes
    execute_nodes_flow_wo = execute_nodes_flow.with_options(
        on_completion=[
            partial(execute_nodes_flow_hook, **dict(generated_nodes=generated_nodes, sorted_nodes=sorted_nodes, testmode=testmode))],
        on_failure=[
            partial(execute_nodes_flow_hook, **dict(generated_nodes=generated_nodes, sorted_nodes=sorted_nodes, testmode=testmode))]
    )

    n = execute_nodes_flow_wo(generated_nodes, sorted_nodes, testmode)  # flow

    # Persist nodes' results in following cases
    # 1. On test mode when trace is enabled
    # 2. Executions in non-test mode
    if ((testmode is False) or (testmode is True and tracemode is True)):
        persist_results_flow(n, trace_config, root_flow_run_id)

    if _options["trace_config"]["trace_mode"]:
        for k in n.keys():
            nodes_out[k] = n[k]
    # return json.dumps(nodes_out) # use return if persisting prefect flow results
    
def execute_subflow_cluster(node_graph, input, test):
    prefect_dask_module = importlib.import_module("prefect_dask")
    scheduler_address = get_scheduler_address(node_graph)
    executor_type = node_graph["nodeobj"].executor_type

    @flow(task_runner=prefect_dask_module.DaskTaskRunner(cluster_kwargs={"processes": False}), log_prints=True)
    def execute_local_cluster(node_graph, input, test):
        return submit_tasks_to_runner(node_graph, input, test)

    @flow(task_runner=prefect_dask_module.DaskTaskRunner(address=scheduler_address), log_prints=True)
    def execute_kube_cluster(node_graph, input, test):
        return submit_tasks_to_runner(node_graph, input, test)

    @flow(task_runner=prefect_dask_module.DaskTaskRunner(address=scheduler_address), log_prints=True)
    def execute_mpi_cluster(node_graph, input, test):
        return submit_tasks_to_runner(node_graph, input, test)

    def submit_tasks_to_runner(node_graph, input, test):
        subflow_results = OrderedDict()
        count = 0
        for nodename in node_graph["nodeobj"].sorted_nodes:
            # if it is the first node in the subflow
            if count == 0:
                _input = input
            else:
                _input = get_incoming_edges(
                    node_graph["graph"], subflow_results, nodename)

            node = node_graph["graph"]["nodes"][nodename]

            node_task_execution_wo = execute_node_task.with_options(
                on_completion=[
                    partial(node_task_execution_hook, **dict(nodename=nodename, nodetype=node["type"], nodeobj=node["nodeobj"], input=_input, istest=test))],
                on_failure=[
                    partial(node_task_execution_hook, **dict(nodename=nodename, nodetype=node["type"], nodeobj=node["nodeobj"], input=_input, istest=test))]
            )

            subflow_results[nodename] = node_task_execution_wo.submit(
                nodename, node["type"], node["nodeobj"], _input, test).result()  # Result Obj
            count += 1
        return subflow_results

    match executor_type:
        case "kubernetes":
            subflow_execution_wo = execute_kube_cluster.with_options(
                on_completion=[partial(
                    subflow_execution_hook, **dict(node_graph=node_graph, input=input, istest=test))],
                on_failure=[partial(
                    subflow_execution_hook, **dict(node_graph=node_graph, input=input, istest=test))]
            )
        case "mpi":
            subflow_execution_wo = execute_mpi_cluster.with_options(
                on_completion=[partial(
                    subflow_execution_hook, **dict(node_graph=node_graph, input=input, istest=test))],
                on_failure=[partial(
                    subflow_execution_hook, **dict(node_graph=node_graph, input=input, istest=test))]
            )
        case "default":
            subflow_execution_wo = execute_local_cluster.with_options(
                on_completion=[partial(
                    subflow_execution_hook, **dict(node_graph=node_graph, input=input, istest=test))],
                on_failure=[partial(
                    subflow_execution_hook, **dict(node_graph=node_graph, input=input, istest=test))]
            )

    return subflow_execution_wo(node_graph, input, test)


@flow(name="execute-nodes",
      flow_run_name="execute-nodes-flowrun",
      log_prints=True)
def execute_nodes_flow(graph, sorted_nodes, test):
    nodes = {}
    try:
        for nodename in sorted_nodes:
            node = graph["nodes"][nodename]
            _input = get_incoming_edges(graph, nodes, nodename)
            if node["type"] not in [
                "csv_node",
                "db_reader_node",
                "sql_node",
                "python_node",
                "db_writer_node",
                "sql_query_node",
                "r_node",
                "data_mapping_node",
                "subflow", 
                "time_at_risk_node",
                "cohort_generator_node",
                "cohort_diagnostic_node",
                "characterization_node", 
                "negative_control_outcome_cohort_node",
                "target_comparator_outcomes_node",
                "cohort_method_analysis_node",
                "default_covariate_settings_node",
                "study_population_settings_node",
                "cohort_incidence_target_cohorts_node",
                "cohort_incidence_node",
                "cohort_definition_set_node",
                "outcomes_node",
                "cohort_method_node",
                "era_covariate_settings_node",
                "seasonality_covariate_settings_node",
                "calendar_time_covariate_settings_node",
                "study_population_settings_node",
                "nco_cohort_set_node",
                "self_controlled_case_series_analysis_node",
                "self_controlled_case_series_node",
                "patient_level_prediction_node",
                "exposure_node",
                "strategus_node"
            ]:
                get_run_logger().error(f"gen.py: execute_nodes: {node['type']} Node Type not known")
            else: 
                if node["type"] == "subflow":
                    # execute as a subflow with runner
                    result_of_subflow = execute_subflow_cluster(
                        node, _input, test)

                    # output of subflow
                    nodes[nodename] = result_of_subflow.popitem(
                        True)[1]  # result is an ordered dict

                    # also save results of tasks in subflow
                    for subflow_node in result_of_subflow:
                        nodes[subflow_node] = result_of_subflow[subflow_node]
                else:
                    node_task_execution_wo = execute_node_task.with_options(
                        on_completion=[
                            partial(node_task_execution_hook, **dict(nodename=nodename, nodetype=node["type"], nodeobj=node["nodeobj"], input=_input, istest=test))],
                        on_failure=[
                            partial(node_task_execution_hook, **dict(nodename=nodename, nodetype=node["type"], nodeobj=node["nodeobj"], input=_input, istest=test))]
                    )

                    nodes[nodename] = node_task_execution_wo(
                        nodename, node["type"], node["nodeobj"], _input, test)
    except Exception as e:
        get_run_logger().error(traceback.format_exc())
    return nodes


@task(task_run_name="execute-nodes-taskrun-{nodename}",
      result_storage=RFS.load(os.getenv("DATAFLOW_MGMT__FLOWS__RESULTS_SB_NAME")), 
      result_storage_key="{flow_run.id}_{parameters[nodename]}.json",
      result_serializer=JSONSerializer(object_encoder="nodes.nodes.serialize_result_to_json"), log_prints=True,
      persist_result=True)
def execute_node_task(nodename, node_type, node, input, test):
    # Get task run context
    task_run_context = TaskRunContext.get().task_run.dict()

    _node = node
    result = None
    if test:
        result = _node.test(task_run_context)
    else:
        match node_type:
            case ('db_reader_node' | 'csv_node' | 'cohort_diagnostic_node' | 'calendar_time_covariate_settings_node' |
                'cohort_generator_node' | 'time_at_risk_node' | 'default_covariate_settings_node' | 
                'study_population_settings_node' | 'cohort_incidence_target_cohorts_node' | 'cohort_definition_set_node' | 
                'era_covariate_settings_node' | 'seasonality_covariate_settings_node' | 'nco_cohort_set_node'):
                result = _node.task(task_run_context)
            case _:
                result = _node.task(input, task_run_context)
    return result


@task(task_run_name="persist-result-taskrun-{nodename}",
      log_prints=True)
def persist_node_task(nodename, result, trace_db, root_flow_run_id):
    result_json = {}
    result_json["result"] = serialize_to_json(result.data)
    
    dbutils_module = importlib("utils.DBUtils")
    results_db_engine = dbutils_module.GetConfigDBConnection()

    with results_db_engine.connect() as connection:
        try:
            metadata = sql.MetaData(schema="dataflow")
            result_table = sql.Table("dataflow_result", metadata,
                                 autoload_with=connection, schema="dataflow")
            insert_stmt = sql.insert(result_table).values(
                node_name=nodename,
                flow_run_id=result.flow_run_id,
                task_run_id=result.task_run_id,
                root_flow_run_id=root_flow_run_id,
                task_run_result=result_json,
                created_by="xyz",
                modified_by="xyz",
                error=result.error,
                error_message=result.data if result.error else None
            )
            connection.execute(insert_stmt)
            connection.commit()
        except Exception as e:
            get_run_logger().error(
                f"Failed to persist results for task run '{result.task_run_name}': {e}")
        else:
            get_run_logger().info(
                f"Successfully persisted results for task run '{result.task_run_name}'")


@flow(name="persist-results",
      flow_run_name="persist-results-flowrun",
      log_prints=True)
def persist_results_flow(nodes, trace, root_flow_run_id):
    get_run_logger().debug(f"To check results nodes: {nodes}")
    try:
        for name in nodes:

            # unpack ordered dict
            if isinstance(nodes[name], OrderedDict):
                ordered_dict_result = nodes[name]
                for node in ordered_dict_result:
                    result = ordered_dict_result[node].data
                    persist_node_task(
                        name, result, trace["trace_db"], root_flow_run_id)  # task
            else:
                result = nodes[name]
                persist_node_task(
                    name, result, trace["trace_db"], root_flow_run_id)  # task
    except Exception as e:
        get_run_logger().error(e)

